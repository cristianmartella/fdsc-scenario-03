---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: provider-dsc-etcd
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
    app.kubernetes.io/component: etcd
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 2379
        - port: 2380
---
# Source: data-space-connector/charts/apisix/templates/control-plane/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: provider-dsc-apisix-control-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: control-plane
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 9180
        - port: 9280
        - port: 9090
        - port: 9099
---
# Source: data-space-connector/charts/apisix/templates/data-plane/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: provider-dsc-apisix-data-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: data-plane
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 9080
        - port: 9443
        - port: 9090
        - port: 9099
---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: provider-dsc-etcd
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
    app.kubernetes.io/component: etcd
spec:
  minAvailable: 51%
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
---
# Source: data-space-connector/charts/apisix/templates/control-plane/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: provider-dsc-apisix-control-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: control-plane
---
# Source: data-space-connector/charts/apisix/templates/dashboard/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: provider-dsc-apisix-dashboard
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.0.1
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: dashboard
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: dashboard
---
# Source: data-space-connector/charts/apisix/templates/data-plane/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: provider-dsc-apisix-data-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: data-plane
---
# Source: data-space-connector/charts/apisix/templates/ingress-controller/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: provider-dsc-apisix-ingress-controller
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 1.8.2
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: ingress-controller
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: ingress-controller
---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: provider-dsc-etcd
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
---
# Source: data-space-connector/charts/apisix/templates/control-plane/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: provider-dsc-apisix-control-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
automountServiceAccountToken: false
---
# Source: data-space-connector/charts/apisix/templates/dashboard/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: provider-dsc-apisix-dashboard
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.0.1
    helm.sh/chart: apisix-3.5.1
automountServiceAccountToken: false
---
# Source: data-space-connector/charts/apisix/templates/data-plane/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: provider-dsc-apisix-data-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
automountServiceAccountToken: false
---
# Source: data-space-connector/charts/apisix/templates/ingress-controller/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: provider-dsc-apisix-ingress-controller
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 1.8.2
    helm.sh/chart: apisix-3.5.1
automountServiceAccountToken: false
---
# Source: data-space-connector/charts/mysql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: authentication-mysql
  namespace: "provider"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.4
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.0.31"
  annotations:
automountServiceAccountToken: true
secrets:
  - name: authentication-database-secret
---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/token-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: provider-dsc-etcd-jwt-token
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
type: Opaque
data:
  jwt-token.pem: "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"
---
# Source: data-space-connector/charts/apisix/templates/control-plane/api-token-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: provider-dsc-apisix-control-plane-api-token
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
type: Opaque
data:
  admin-token: "cW1xRW1CVlRHMlNJRVRxZVZTZlc3NTVjV09GUnlPVHI="
  viewer-token: "OWoxclJVODhKdzlqSFIxTEZqQzJzeUV6VnpETmNNY2c="
---
# Source: data-space-connector/charts/apisix/templates/tls-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: provider-dsc-apisix-control-plane-tls
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQrVENDQXVHZ0F3SUJBZ0lRZXVKT2NHcDBWU1hENU04MzlSQjlQekFOQmdrcWhraUc5dzBCQVFzRkFEQVUKTVJJd0VBWURWUVFERXdsaGNHbHphWGd0WTJFd0hoY05NalV3TlRJNU1UTXhPRE15V2hjTk1qWXdOVEk1TVRNeApPRE15V2pBc01Tb3dLQVlEVlFRREV5RndjbTkyYVdSbGNpMWtjMk10WVhCcGMybDRMV052Ym5SeWIyd3RjR3hoCmJtVXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDNHdiVXorck9yQkYrVXZUeSsKclBuanpSTnNPN3JpSGtFaGNjaTZLaEQ1MTdna3h2d3JvU0FKOFg1eFNjQUNqY2prQUM1WXRBZG9PVEFSaUFicApzdUpMVVJRa1V4eXdKOTYvOE9Wd3BMcklJMzg5bGdBS2JFeVJkQW01Umx1NHAxYWZmS29NNURsY1pMOXVQMjNxCjdmQk1iRHQ5N1dMWW4rNm8reU5JT095RWNEdzBuMWRYYXMyRGlDUzRwZUVQT2ZUY2ErOXpBY2RhdW1hbVFuek0KQ3hPRXhCSlI2aFQyY1B6and0U2QyY0h1Um0wWTZRclg3VnpUb1hDSHRNYXpNZTBWT2U4RXVmMVU0SjY5SGZaRgo0Y2E2MVNHY2hZWmg3L2V5R1RGeko2bm5yc0dTaGtTaWdOMVhreDJSOHgreGtGTDh5b212V0tsWEExbnFLQVRPCm5QR1hBZ01CQUFHamdnRXRNSUlCS1RBT0JnTlZIUThCQWY4RUJBTUNCYUF3SFFZRFZSMGxCQll3RkFZSUt3WUIKQlFVSEF3RUdDQ3NHQVFVRkJ3TUNNQXdHQTFVZEV3RUIvd1FDTUFBd0h3WURWUjBqQkJnd0ZvQVVOdHZwd1RucwpPM3lJSGdYTThLSmVoWGNRR1gwd2djZ0dBMVVkRVFTQndEQ0J2WUloY0hKdmRtbGtaWEl0WkhOakxXRndhWE5wCmVDMWpiMjUwY205c0xYQnNZVzVsZ2lwd2NtOTJhV1JsY2kxa2MyTXRZWEJwYzJsNExXTnZiblJ5YjJ3dGNHeGgKYm1VdWNISnZkbWxrWlhLQ0xuQnliM1pwWkdWeUxXUnpZeTFoY0dsemFYZ3RZMjl1ZEhKdmJDMXdiR0Z1WlM1dwpjbTkyYVdSbGNpNXpkbU9DUEhCeWIzWnBaR1Z5TFdSell5MWhjR2x6YVhndFkyOXVkSEp2YkMxd2JHRnVaUzV3CmNtOTJhV1JsY2k1emRtTXVZMngxYzNSbGNpNXNiMk5oYkRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQVJTdW0KNHYxVzRxcGR5SkdKcU10bXo3Z3pObUJhYWJRVG5pSVY2S2xlVVBmcTNXNjBjT2t3NzE3dWsvWCtQZlFZdmg4NgpZTHFtcUw5N1RsNGU0ZS9lWVZsZU94bXhWTlQ1NHlYb0t1emZCVXp6QXlqUXp3RWk0V0tnbW9FaFZ0cVpzaUZJCk80ZmZlQkwvMXNZTStvYWhJOHRQRFR5Y3FYUWtuNGY2RFBRUVJOSWZRaGdWY0lGYkRnZzdyRGh2QjR1UGJRLzMKcUk2Qk4vS2NJeExwcmtLYjY0aVI3OHdwNW1NLzFnc3ZHbWI3NitNSHZLZW5xUDUwa1B3L0Q2Q0FTK1Z0dzBaVAowUDdKWTh4b3A4cHc1ZHRmbnhxSy9JWjNOZVhGQTF6YmhaYnZqT2Y3UkFackYxbnhPVzNWYUtieWZUU3dDYk45Clp0eThCc3lzdWhoa1lYRlh1dz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
  tls.key: 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
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGRENDQWZ5Z0F3SUJBZ0lSQUpjQmxLV1Z4ZnhkT1VaR2FZK2FFcjB3RFFZSktvWklodmNOQVFFTEJRQXcKRkRFU01CQUdBMVVFQXhNSllYQnBjMmw0TFdOaE1CNFhEVEkxTURVeU9URXpNVGd6TWxvWERUSTJNRFV5T1RFegpNVGd6TWxvd0ZERVNNQkFHQTFVRUF4TUpZWEJwYzJsNExXTmhNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DCkFROEFNSUlCQ2dLQ0FRRUFuRXlacTRRVGVEeVl3aVkrSlRVdkR3SmJtV3lhWkppb3RuZ3hHV1pXckJMeldwZmsKd0NXUkZCM3JaSWVKbkh1REplbEhaWGhsQ0pCWkk3eGZ3M0ZpRDd5cTBhY1VzeVFVQTdSaW1xWlVTY1JRUEZRSApRZEtBWWNoeTJ5bS8zK3Y2UlpnNjhweXBkM1JzVXpQUnJuaHd0a2xqa3FpRWdIa3RWTUMvRDJOcjNSTzYzS0daCndDT1hLb29IZUZVaGY3MTJya09pRTdYazJZd2thSHRUYWszWG8vTm15emdnSEVLK0hXeHRKL0p1L08wbFE2SHUKcERCRnpTdlZIQVZEUU1ZZTFHRUU1RUxPdkdFR0RmNGlWbGZISHBEZ1NXU0krZDBXZG1KNmFQaVZYMjhtczN0awpUVmVGSW1aVklEdUo0d0Q1czZwM1pCSGEyd1hPQlVhdjVjQ3BDUUlEQVFBQm8yRXdYekFPQmdOVkhROEJBZjhFCkJBTUNBcVF3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUYKTUFNQkFmOHdIUVlEVlIwT0JCWUVGRGJiNmNFNTdEdDhpQjRGelBDaVhvVjNFQmw5TUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQ1QxOWJXUzhmM0tvOHIyck1QRU9vejVjVFd6TDNTTnkxWmdseHIzME81MXdoSVFKek5YWkJUCisyZ3FqMHdFYjFQM04vTjNIUEVOdTdPYkhra2JQajJsZmxidHN5cWZ4WXQvMEtka1J5TkgyTVE2bHRxUjFvQmcKNlZFUWo2TDhMMUZSQ0R3amROK0RRWElXM0NuR1Q4YVc2TVB4VjlVWDYyOG1Zd3V1cWNUYmhpN0xBNUVPdVl4cgpFMWQvbkwyaE9CazZEMEdYbDNmUWIxSUt6bjNvbFNXaWtiYkRSbG41Z0hhMVRPeHp4OVNQZ25PVlFrYXN4ekczClgxYk95VGQ1TTF3NFJSQUNLcy9LL2FKa2owaWpSQzJpdzNvbTMvdW54ZzV1STArQkd5SmV5cnhNMFMxN2lUeXIKelZhdjdMa05PazVqYVdacm1uNnJ6MTJMODNsT0RzZHMKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
---
# Source: data-space-connector/charts/apisix/templates/tls-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: provider-dsc-apisix-data-plane-tls
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ2ekNDQXRPZ0F3SUJBZ0lSQU5ScnoyZnJQbVZ6eTBoa3ZWRTkwSFF3RFFZSktvWklodmNOQVFFTEJRQXcKRkRFU01CQUdBMVVFQXhNSllYQnBjMmw0TFdOaE1CNFhEVEkxTURVeU9URXpNVGd6TWxvWERUSTJNRFV5T1RFegpNVGd6TWxvd0tURW5NQ1VHQTFVRUF4TWVjSEp2ZG1sa1pYSXRaSE5qTFdGd2FYTnBlQzFrWVhSaExYQnNZVzVsCk1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBcmZhdGh2VGl5Q1g1R2U0SWVmcHQKUGNPc2E1VGlRRWw4eWVzVGc4emtKemplSmoxNStsd2lQZFUzZkJnTCtYN3d3L1loK1ZQUjQvVThvZUROSmsxawppOG4vQ3VsbHdOd0F6OEQvUENHU3h2cUVwVEdFSDFqZkxWL2MrSDM1RC81dTd6aVBMOFppMzd5S3JHY1QxTVVMCkZ1ZWdtSzgvNlU2cW03N0RmcjlFcUNFRGJWa2pKWXBOUEhjTWpZNVBxK3VGSW15b2JtQnFUa0p3Z3U0SWN6amUKek9DMmZTRDkzQ1RJZFdTR1grMDNCdERpRjFoNzBqOHh3clhOMUozVGxydEgwTklaSjVZTzNGL3NzUzNOSUwwZwo1ejkyWlQrWHVKSUMyMjFTbTQ1ZWs5UExMQzFWUFFDUVRsWXpZRXNXbHB2Q2FLMW5JQVNpZmJaclJNb052UU1SCnR3SURBUUFCbzRJQklUQ0NBUjB3RGdZRFZSMFBBUUgvQkFRREFnV2dNQjBHQTFVZEpRUVdNQlFHQ0NzR0FRVUYKQndNQkJnZ3JCZ0VGQlFjREFqQU1CZ05WSFJNQkFmOEVBakFBTUI4R0ExVWRJd1FZTUJhQUZEYmI2Y0U1N0R0OAppQjRGelBDaVhvVjNFQmw5TUlHOEJnTlZIUkVFZ2JRd2diR0NIbkJ5YjNacFpHVnlMV1J6WXkxaGNHbHphWGd0ClpHRjBZUzF3YkdGdVpZSW5jSEp2ZG1sa1pYSXRaSE5qTFdGd2FYTnBlQzFrWVhSaExYQnNZVzVsTG5CeWIzWnAKWkdWeWdpdHdjbTkyYVdSbGNpMWtjMk10WVhCcGMybDRMV1JoZEdFdGNHeGhibVV1Y0hKdmRtbGtaWEl1YzNaagpnamx3Y205MmFXUmxjaTFrYzJNdFlYQnBjMmw0TFdSaGRHRXRjR3hoYm1VdWNISnZkbWxrWlhJdWMzWmpMbU5zCmRYTjBaWEl1Ykc5allXd3dEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ25WQXNZa2ZUUWJHc04xQ3ZiR01VTjQKZUdudGU3eXB5aGlpd2liYm5mdFJ5SmNYOXpjdnd2YlZoZTQ2MFlkZloyV1Y3TEh4YjM1Qy8rTFMyUzVPZXVNVApmanozR1ZuSklXUklIaThtQ0dMU1hKeXVqN3N2bW93NS90RTg1b2thOWtuMi9XKzMwZlljWTFaYXdWZ3I5K1dZCkZsK2VYcUhzQklrWXpJcW5QMy9NTWxxUTZEUzR3SkZST29tY2hFejlwQU9HcU80bll3KzU2YmVlTTFRejFCWEsKU0NQTTBBRnB3UTZ5ZzA0MjNsd1FMRU0yNmVJSXZPWDdaLzQzNzBKajdoSVBRdHNaNldpSjNZRVJYdjJWWjYvMgppaGsydHBFT1lZVVRoWkVMM1RGL0dERU5kNjd0ZEdUR1lORW43YmlQMytjUmhoMXRESWp3K0F4QnRrcTdmN1U9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
  tls.key: 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
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGRENDQWZ5Z0F3SUJBZ0lSQUpjQmxLV1Z4ZnhkT1VaR2FZK2FFcjB3RFFZSktvWklodmNOQVFFTEJRQXcKRkRFU01CQUdBMVVFQXhNSllYQnBjMmw0TFdOaE1CNFhEVEkxTURVeU9URXpNVGd6TWxvWERUSTJNRFV5T1RFegpNVGd6TWxvd0ZERVNNQkFHQTFVRUF4TUpZWEJwYzJsNExXTmhNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DCkFROEFNSUlCQ2dLQ0FRRUFuRXlacTRRVGVEeVl3aVkrSlRVdkR3SmJtV3lhWkppb3RuZ3hHV1pXckJMeldwZmsKd0NXUkZCM3JaSWVKbkh1REplbEhaWGhsQ0pCWkk3eGZ3M0ZpRDd5cTBhY1VzeVFVQTdSaW1xWlVTY1JRUEZRSApRZEtBWWNoeTJ5bS8zK3Y2UlpnNjhweXBkM1JzVXpQUnJuaHd0a2xqa3FpRWdIa3RWTUMvRDJOcjNSTzYzS0daCndDT1hLb29IZUZVaGY3MTJya09pRTdYazJZd2thSHRUYWszWG8vTm15emdnSEVLK0hXeHRKL0p1L08wbFE2SHUKcERCRnpTdlZIQVZEUU1ZZTFHRUU1RUxPdkdFR0RmNGlWbGZISHBEZ1NXU0krZDBXZG1KNmFQaVZYMjhtczN0awpUVmVGSW1aVklEdUo0d0Q1czZwM1pCSGEyd1hPQlVhdjVjQ3BDUUlEQVFBQm8yRXdYekFPQmdOVkhROEJBZjhFCkJBTUNBcVF3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0dBUVVGQndNQ01BOEdBMVVkRXdFQi93UUYKTUFNQkFmOHdIUVlEVlIwT0JCWUVGRGJiNmNFNTdEdDhpQjRGelBDaVhvVjNFQmw5TUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQ1QxOWJXUzhmM0tvOHIyck1QRU9vejVjVFd6TDNTTnkxWmdseHIzME81MXdoSVFKek5YWkJUCisyZ3FqMHdFYjFQM04vTjNIUEVOdTdPYkhra2JQajJsZmxidHN5cWZ4WXQvMEtka1J5TkgyTVE2bHRxUjFvQmcKNlZFUWo2TDhMMUZSQ0R3amROK0RRWElXM0NuR1Q4YVc2TVB4VjlVWDYyOG1Zd3V1cWNUYmhpN0xBNUVPdVl4cgpFMWQvbkwyaE9CazZEMEdYbDNmUWIxSUt6bjNvbFNXaWtiYkRSbG41Z0hhMVRPeHp4OVNQZ25PVlFrYXN4ekczClgxYk95VGQ1TTF3NFJSQUNLcy9LL2FKa2owaWpSQzJpdzNvbTMvdW54ZzV1STArQkd5SmV5cnhNMFMxN2lUeXIKelZhdjdMa05PazVqYVdacm1uNnJ6MTJMODNsT0RzZHMKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
---
# Source: data-space-connector/templates/authentication-secrets.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: authentication-database-secret
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  mysql-root-password: "STQzZVBGdmpRTnRWRFpwTTAwWUhTb0FYM09xUlkw"
  mysql-replication-password: "NDBJV2N1bFlXeENZdjVwaWM2bEdRSWJJRFlVRDJD"
  mysql-password: "TmpCdjh0UnhrVFlJRFJRVHpzSk9jQ1B3cGwyd0pX"
---
# Source: data-space-connector/templates/data-plane-secrets.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: data-service-secret
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  postgres-user-password: "Rm9xOEdZZngxZGFnSXhVNVpZSjRWQUJEbUpQRWdj"
  postgres-admin-password: "dWJiOExKa2JxUTcyYmJ3NzZVdzB1T2xjelV2WXl1"
---
# Source: data-space-connector/templates/database-secrets.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: database-secret
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  postgres-user-password: "WjhlZkdiMmR0cE5LdDR1d08yMnpPeUR0MVp5dXVr"
  postgres-admin-password: "SDU1OTlub1FvQTBTWml2NWVzQ21UbUxURjN0MlZS"
---
# Source: data-space-connector/templates/issuance-secrets.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: issuance-secret
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  postgres-user-password: "MGltTHUxcmF2VXNqb1c2RDNmYU53czJodWhlczZP"
  postgres-admin-password: "QkcxaWNndXNzU09IUkp5cm9GcGlvRkVwR1drMUN1"
  keycloak-admin: "MkNuSENveFB2bm5FSXl2cTliVFBlVGFwalVTa09X"
  store-pass: "U2syMHA5eWxtM2I4bzlhTEpUWnlTZTFoeXdYY0xD"
---
# Source: data-space-connector/charts/apisix/templates/control-plane/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: provider-dsc-apisix-control-plane-default
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
data:
  config.yaml: |
    
    nginx_config:
      error_log: /dev/stderr
      stream:
        access_log: /dev/stdout
      http:
        access_log: /dev/stdout
      http_configuration_snippet: |
        proxy_buffering off;
    apisix:
      control:
        ip: 0.0.0.0
        port: 9090
    deployment:
      role: control_plane
      role_control_plane:
          config_provider: etcd
          conf_server:
            listen: 0.0.0.0:9280
            cert: /bitnami/certs/tls.crt
            cert_key: /bitnami/certs/tls.key
      etcd:
        host:
          - http://provider-dsc-etcd-0.provider-dsc-etcd-headless:2379
          - http://provider-dsc-etcd-1.provider-dsc-etcd-headless:2379
          - http://provider-dsc-etcd-2.provider-dsc-etcd-headless:2379
        prefix: /apisix
        timeout: 30
        use_grpc: false
        startup_retry: 60
      certs:
        cert: /bitnami/certs/tls.crt
        cert_key: /bitnami/certs/tls.key
        client_ca_cert: /bitnami/certs/ca.crt
      admin:
        https_admin: true
        admin_api_mtls:
          admin_ssl_cert: /bitnami/certs/tls.crt
          admin_ssl_cert_key: /bitnami/certs/tls.key
    
        allow_admin:
          - 0.0.0.0/0
    
        admin_key:
          - name: admin
            key: "{{APISIX_ADMIN_API_TOKEN}}"
            role: admin
          - name: viewer
            key: "{{APISIX_VIEWER_API_TOKEN}}"
            role: viewer
        admin_listen:
            port: 9180
        enable_admin_cors: true         # Admin API support CORS response headers.
    discovery:
      kubernetes:
        service:
          schema: https #default https
    
          # apiserver host, options [ipv4, ipv6, domain, environment variable]
          host: ${KUBERNETES_SERVICE_HOST}
    
          # apiserver port, options [port number, environment variable]
          port: ${KUBERNETES_SERVICE_PORT}
    
        client:
          # serviceaccount token or token_file
          token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    
        default_weight: 50 # weight assigned to each discovered endpoint. default 50, minimum 0
---
# Source: data-space-connector/charts/apisix/templates/data-plane/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: provider-dsc-apisix-data-plane-default
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
data:
  00_default_config.yaml: |
    
    apisix:
      node_listen: 9080
      enable_admin: false
      ssl:
        enable: true
        listen:
          - port: 9443
        ssl_trusted_certificate: /bitnami/certs/ca.crt
      enable_http2: true
      control:
        ip: 0.0.0.0
        port: 9090
    nginx_config:
      error_log: /dev/stderr
      stream:
        access_log: /dev/stdout
      http:
        access_log: /dev/stdout
      http_configuration_snippet: |
        proxy_buffering off;
    deployment:
      role: data_plane
      role_data_plane:
        config_provider: etcd
        control_plane:
          host:
            - https://provider-dsc-apisix-control-plane:9280
          prefix: /apisix
          timeout: 30
      etcd:
        host:
          - http://provider-dsc-etcd-0.provider-dsc-etcd-headless:2379
          - http://provider-dsc-etcd-1.provider-dsc-etcd-headless:2379
          - http://provider-dsc-etcd-2.provider-dsc-etcd-headless:2379
        prefix: /apisix
        timeout: 30
        use_grpc: false
        startup_retry: 60
      certs:
        cert: /bitnami/certs/tls.crt
        cert_key: /bitnami/certs/tls.key
        client_ca_cert: /bitnami/certs/ca.crt
    discovery:
      kubernetes:
        service:
          # apiserver schema, options [http, https]
          schema: https #default https
    
          # apiserver host, options [ipv4, ipv6, domain, environment variable]
          host: ${KUBERNETES_SERVICE_HOST} #default ${KUBERNETES_SERVICE_HOST}
    
          # apiserver port, options [port number, environment variable]
          port: ${KUBERNETES_SERVICE_PORT}  #default ${KUBERNETES_SERVICE_PORT}
    
        client:
          # serviceaccount token or token_file
          token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    
        default_weight: 50 # weight assigned to each discovered endpoint. default 50, minimum 0
---
# Source: data-space-connector/charts/apisix/templates/data-plane/extra-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: provider-dsc-apisix-data-plane-extra
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
data:
  01_extra-config.yaml: |
    apisix:
      extra_lua_path: /extra/apisix/plugins/?.lua
    deployment:
      role_data_plane:
        config_provider: yaml
---
# Source: data-space-connector/charts/contract-management/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: contract-management
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: contract-management
    helm.sh/chart: contract-management-1.0.2
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.0"
    app.kubernetes.io/managed-by: Helm
data:
  application.yaml: |
    micronaut:
      http:
        services:
          read-timeout: 30s
          party:
            path: /tmf-api/party/v4
            read-timeout: 30
            url: http://tm-forum-api:8080
          product-catalog:
            path: /tmf-api/productCatalogManagement/v4
            read-timeout: 30
            url: http://tm-forum-api:8080
          product-order:
            path: /tmf-api/productOrderingManagement/v4
            read-timeout: 30
            url: http://tm-forum-api:8080
          rainbow:
            path: /
            read-timeout: 30
            url: http://rainbow:8080
          service-catalog:
            path: /tmf-api/serviceCatalogManagement/v4
            read-timeout: 30
            url: http://tm-forum-api:8080
          tmforum-agreement-api:
            path: /tmf-api/agreementManagement/v4
            read-timeout: 30
            url: http://tm-forum-api:8080
          trusted-issuers-list:
            path: ""
            read-timeout: 30
            url: http://trusted-issuers-list:8080
    general:
      host: contract-management
      port: 8080
      til:
        claims:
        - roles:
          - Consumer
          - Admin
          target: did:some:service
        credentialType: OperatorCredential
      
    notification:
      entities:
        - entityType: "ProductOrder"
          eventTypes: [ "CREATE", "STATE_CHANGE", "DELETE" ]
          apiAddress: "http://tm-forum-api:8080/tmf-api/productOrderingManagement/v4"
        - entityType: "ProductOffering"
          eventTypes: [ "CREATE", "STATE_CHANGE", "DELETE" ]
          apiAddress: "http://tm-forum-api:8080/tmf-api/productCatalogManagement/v4"
        - entityType: "Catalog"
          eventTypes: [ "CREATE", "DELETE", "STATE_CHANGE" ]
          apiAddress: "http://tm-forum-api:8080/tmf-api/productCatalogManagement/v4"
---
# Source: data-space-connector/charts/mysql/templates/primary/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: authentication-mysql
  namespace: "provider"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.4
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.0.31"
    app.kubernetes.io/component: primary
data:
  my.cnf: |-
    [mysqld]
    default_authentication_plugin=mysql_native_password
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mysql
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    datadir=/bitnami/mysql/data
    tmpdir=/opt/bitnami/mysql/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    log-error=/opt/bitnami/mysql/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    slow_query_log_file=/opt/bitnami/mysql/logs/mysqld.log
    long_query_time=10.0
    
    [client]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    
    [manager]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
---
# Source: data-space-connector/charts/mysql/templates/primary/initialization-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: authentication-mysql-init-scripts
  namespace: "provider"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.4
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.0.31"
    app.kubernetes.io/component: primary
data:
  create.sql: |
    CREATE DATABASE tildb;
    CREATE DATABASE ccsdb;
---
# Source: data-space-connector/charts/postgis/templates/primary/initialization-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-service-postgis-init-scripts
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: data-service-postgis
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgis-13.1.5
data:
  enable.sh: |
    psql postgresql://postgres:${POSTGRES_PASSWORD}@localhost:5432 -c "CREATE EXTENSION postgis;"
    psql postgresql://postgres:${POSTGRES_PASSWORD}@localhost:5432 -c "CREATE DATABASE ngb;"
---
# Source: data-space-connector/charts/postgresql/templates/primary/initialization-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgresql-init-scripts
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgresql-13.1.5
data:
  create.sh: |
    psql postgresql://postgres:${POSTGRES_PASSWORD}@localhost:5432 -c "CREATE DATABASE pap;"
    psql postgresql://postgres:${POSTGRES_PASSWORD}@localhost:5432 -c "CREATE DATABASE keycloak;"
    psql postgresql://postgres:${POSTGRES_PASSWORD}@localhost:5432 -c "CREATE DATABASE rainbow;"
---
# Source: data-space-connector/charts/tm-forum-api/templates/envoy-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: provider-dsc-tm-forum-api-envoy
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
data:
  envoy-config.yaml: |
    static_resources:
      listeners:
        - name: listener_0
          address:
            socket_address:
              address: 0.0.0.0
              port_value: 10000
          filter_chains:
          - filters: 
            - name: envoy.filters.network.http_connection_manager
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                stat_prefix: ingress_http
                codec_type: AUTO
                route_config:
                  name: tmforum-api
                  virtual_hosts:
                    - name: tmforum-api
                      domains: ["*"]
                      routes:
                        - match: 
                            prefix: "/tmf-api/party/v4"
                          route:
                            cluster: party-catalog
                        - match: 
                            prefix: "/tmf-api/customerBillManagement/v4"
                          route:
                            cluster: customer-bill-management
                        - match: 
                            prefix: "/tmf-api/customerManagement/v4"
                          route:
                            cluster: customer-management
                        - match: 
                            prefix: "/tmf-api/productCatalogManagement/v4"
                          route:
                            cluster: product-catalog
                        - match: 
                            prefix: "/tmf-api/productInventory/v4"
                          route:
                            cluster: product-inventory
                        - match: 
                            prefix: "/tmf-api/productOrderingManagement/v4"
                          route:
                            cluster: product-ordering-management
                        - match: 
                            prefix: "/tmf-api/resourceCatalog/v4"
                          route:
                            cluster: resource-catalog
                        - match: 
                            prefix: "/tmf-api/resourceFunctionActivation/v4"
                          route:
                            cluster: resource-function-activation
                        - match: 
                            prefix: "/tmf-api/resourceInventoryManagement/v4"
                          route:
                            cluster: resource-inventory
                        - match: 
                            prefix: "/tmf-api/serviceCatalogManagement/v4"
                          route:
                            cluster: service-catalog
                        - match: 
                            prefix: "/tmf-api/agreementManagement/v4"
                          route:
                            cluster: agreement                        
                http_filters:
                  - name: http_router
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
      clusters:
        - name: party-catalog
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: party-catalog
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-party-catalog
                          port_value: 8080
        - name: customer-bill-management
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: customer-bill-management
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-customer-bill-management
                          port_value: 8080
        - name: customer-management
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: customer-management
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-customer-management
                          port_value: 8080
        - name: product-catalog
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: product-catalog
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-product-catalog
                          port_value: 8080
        - name: product-inventory
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: product-inventory
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-product-inventory
                          port_value: 8080
        - name: product-ordering-management
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: product-ordering-management
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-product-ordering-management
                          port_value: 8080
        - name: resource-catalog
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: resource-catalog
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-resource-catalog
                          port_value: 8080
        - name: resource-function-activation
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: resource-function-activation
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-resource-function-activation
                          port_value: 8080
        - name: resource-inventory
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: resource-inventory
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-resource-inventory
                          port_value: 8080
        - name: service-catalog
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: service-catalog
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-service-catalog
                          port_value: 8080
        - name: agreement
          connect_timeout: 15s
          type: STRICT_DNS
          dns_lookup_family: V4_ONLY
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: agreement
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          protocol: TCP
                          address: provider-dsc-tm-forum-api-agreement
                          port_value: 8080
---
# Source: data-space-connector/charts/vcverifier/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: verifier
  namespace: "provider"
  labels:
    app.kubernetes.io/name: vcverifier
    helm.sh/chart: vcverifier-2.13.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.1"
    app.kubernetes.io/managed-by: Helm
data:
  server.yaml: |-
    server:
      host: http://provider-verifier.127.0.0.1.nip.io:8080
      port: 3000
      staticDir: views/static
      templateDir: views/

    m2m:
      authEnabled: false
      keyType: RSAPS256
      
    logging:
      jsonLogging: true
      level: DEBUG
      logRequests: true
      pathsToSkip:
      - /metrics
      - /health

    verifier:
      did: did:key:zDnaee6Q14pEC1F9e4buW5PdQMDpq3NCTDUycBQ6PuLyri1oA
      sessionExpiry: 30
      tirAddress: http://tir.127.0.0.1.nip.io:8080/
      validationMode: none
    
    configRepo:
      configEndpoint: http://credentials-config-service:8080

    elsi:
---
# Source: data-space-connector/templates/apisix-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apisix-routes
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  apisix.yaml: |-
    routes:
      
      - uri: /.well-known/openid-configuration
        host: mp-data-service.127.0.0.1.nip.io
        upstream:
          nodes:
            verifier:3000: 1
          type: roundrobin
        plugins:
          proxy-rewrite:
              uri: /services/data-service/.well-known/openid-configuration
      
      - uri: /.well-known/data-space-configuration
        host: mp-data-service.127.0.0.1.nip.io
        upstream:
          nodes:
            dsconfig:3002: 1
          type: roundrobin
        plugins:
          proxy-rewrite:
            uri: /.well-known/data-space-configuration/data-space-configuration.json
          response-rewrite:
            headers:
              set:
                content-type: application/json
      
      - uri: /*
        host: mp-data-service.127.0.0.1.nip.io
        upstream:
          nodes:
            data-service-scorpio:9090: 1
          type: roundrobin
        plugins:
          # verify the jwt at the verifiers endpoint
          openid-connect:
            bearer_only: true
            use_jwks: true
            client_id: data-service
            client_secret: unused
            ssl_verify: false
            discovery: http://verifier:3000/services/data-service/.well-known/openid-configuration
          # request decisions at opa
          opa:
            host: "http://localhost:8181"
            policy: policy/main
            with_body: true
      
      - uri: /.well-known/openid-configuration
        host: mp-tmf-api.127.0.0.1.nip.io
        upstream:
          nodes:
            verifier:3000: 1
          type: roundrobin
        plugins:
          proxy-rewrite:
            uri: /services/tmf-api/.well-known/openid-configuration
      
      - uri: /*
        host: mp-tmf-api.127.0.0.1.nip.io
        upstream:
          nodes:
            tm-forum-api:8080: 1
          type: roundrobin
        plugins:
          openid-connect:
            bearer_only: true
            use_jwks: true
            client_id: contract-management
            client_secret: unused
            ssl_verify: false
            discovery: http://verifier:3000/services/tmf-api/.well-known/openid-configuration
          opa:
            host: "http://localhost:8181"
            policy: policy/main
            with_body: true
      
      - uri: /.well-known/openid-configuration
        host: rustapitest.127.0.0.1.nip.io
        upstream:
          nodes:
            verifier:3000: 1
          type: roundrobin
        plugins:
          proxy-rewrite:
              uri: /services/rustapitest-service/.well-known/openid-configuration
      
      - uri: /.well-known/data-space-configuration
        host: rustapitest.127.0.0.1.nip.io
        upstream:
          nodes:
            dsconfig:3002: 1
          type: roundrobin
        plugins:
          proxy-rewrite:
            uri: /.well-known/data-space-configuration/data-space-configuration.json
          response-rewrite:
            headers:
              set:
                content-type: application/json
      
      - uri: /*
        host: rustapitest.127.0.0.1.nip.io
        upstream:
          nodes:
            provider-dsc-apisix-control-plane:8085: 1
          type: roundrobin
        plugins:
          # verify the jwt at the verifiers endpoint
          openid-connect:
            bearer_only: true
            use_jwks: true
            client_id: rustapitest-service
            client_secret: unused
            ssl_verify: false
            discovery: http://verifier:3000/services/rustapitest-service/.well-known/openid-configuration
          # request decisions at opa
          opa:
            host: "http://localhost:8181"
            policy: policy/main
            with_body: true
    #END
---
# Source: data-space-connector/templates/dataplane-registration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: scorpio-registration
  namespace: "provider"
  labels:
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  init.sh: |-
    # credentials config service registration
    curl -X 'POST' \
      'http://credentials-config-service:8080/service' \
      -H 'accept: */*' \
      -H 'Content-Type: application/json' \
      -d '{
      "id": "data-service",
      "defaultOidcScope": "default","oidcScopes":{"default":[{"trustedIssuersLists":["http://trusted-issuers-list:8080"],"trustedParticipantsLists":["http://tir.trust-anchor.svc.cluster.local:8080"],"type":"UserCredential"}],"operator":[{"trustedIssuersLists":["http://trusted-issuers-list:8080"],"trustedParticipantsLists":["http://tir.trust-anchor.svc.cluster.local:8080"],"type":"OperatorCredential"}]}
    }'
---
# Source: data-space-connector/templates/dsconfig-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dsconfig
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  data-space-configuration.json: |-
    {
      "supported_models": ["https://raw.githubusercontent.com/smart-data-models/dataModel.Consumption/master/ConsumptionPoint/schema.json","https://raw.githubusercontent.com/smart-data-models/dataModel.Consumption/master/ConsumptionCost/schema.json"],
      "supported_protocols": ["http","https"],
      "authentication_protocols": ["oid4vp"]
    }
---
# Source: data-space-connector/templates/opa-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata: 
  name: opa-config
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  opa.yaml: |-
    services:
      - name: bundle-server
        url: http://odrl-pap:8080/bundles/service/v1
    bundles:
      policies:
          service: bundle-server
          resource: policies.tar.gz
          polling:
            min_delay_seconds: 2
            max_delay_seconds: 4
      methods:
          service: bundle-server
          resource: methods.tar.gz
          polling:
            min_delay_seconds: 1
            max_delay_seconds: 3
      data:
          service: bundle-server
          resource: data.tar.gz
          polling:
            min_delay_seconds: 1
            max_delay_seconds: 15
    default_decision: /policy/main/allow
---
# Source: data-space-connector/templates/opa.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opa-lua
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  # extends the apisix opa-plugin to forward the http-body as part of the decision request.
  opa.lua: |-
    --
    -- Licensed to the Apache Software Foundation (ASF) under one or more
    -- contributor license agreements.  See the NOTICE file distributed with
    -- this work for additional information regarding copyright ownership.
    -- The ASF licenses this file to You under the Apache License, Version 2.0
    -- (the "License"); you may not use this file except in compliance with
    -- the License.  You may obtain a copy of the License at
    --
    --     http://www.apache.org/licenses/LICENSE-2.0
    --
    -- Unless required by applicable law or agreed to in writing, software
    -- distributed under the License is distributed on an "AS IS" BASIS,
    -- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    -- See the License for the specific language governing permissions and
    -- limitations under the License.
    --

    local core   = require("apisix.core")
    local http   = require("resty.http")
    local helper = require("apisix.plugins.opa.helper")
    local type   = type
    local ipairs = ipairs

    local schema = {
        type = "object",
        properties = {
            host = {type = "string"},
            ssl_verify = {
                type = "boolean",
                default = true,
            },
            policy = {type = "string"},
            timeout = {
                type = "integer",
                minimum = 1,
                maximum = 60000,
                default = 3000,
                description = "timeout in milliseconds",
            },
            keepalive = {type = "boolean", default = true},
            send_headers_upstream = {
                type = "array",
                minItems = 1,
                items = {
                    type = "string"
                },
                description = "list of headers to pass to upstream in request"
            },
            keepalive_timeout = {type = "integer", minimum = 1000, default = 60000},
            keepalive_pool = {type = "integer", minimum = 1, default = 5},
            with_route = {type = "boolean", default = false},
            with_service = {type = "boolean", default = false},
            with_consumer = {type = "boolean", default = false},
            with_body = {type = "boolean", default = false},
        },
        required = {"host", "policy"}
    }


    local _M = {
        version = 0.1,
        priority = 2001,
        name = "opa",
        schema = schema,
    }


    function _M.check_schema(conf)
        return core.schema.check(schema, conf)
    end


    function _M.access(conf, ctx)
        local body = helper.build_opa_input(conf, ctx, "http")

        local params = {
            method = "POST",
            body = core.json.encode(body),
            headers = {
                ["Content-Type"] = "application/json",
            },
            keepalive = conf.keepalive,
            ssl_verify = conf.ssl_verify
        }

        if conf.keepalive then
            params.keepalive_timeout = conf.keepalive_timeout
            params.keepalive_pool = conf.keepalive_pool
        end

        local endpoint = conf.host .. "/v1/data/" .. conf.policy

        local httpc = http.new()
        httpc:set_timeout(conf.timeout)

        local res, err = httpc:request_uri(endpoint, params)

        -- block by default when decision is unavailable
        if not res then
            core.log.error("failed to process OPA decision, err: ", err)
            return 403
        end

        -- parse the results of the decision
        local data, err = core.json.decode(res.body)

        if not data then
            core.log.error("invalid response body: ", res.body, " err: ", err)
            return 503
        end

        if not data.result then
            core.log.error("invalid OPA decision format: ", res.body,
                           " err: `result` field does not exist")
            return 503
        end

        local result = data.result

        if not result.allow then
            if result.headers then
                core.response.set_header(result.headers)
            end

            local status_code = 403
            if result.status_code then
                status_code = result.status_code
            end

            local reason = nil
            if result.reason then
                reason = type(result.reason) == "table"
                    and core.json.encode(result.reason)
                    or result.reason
            end

            return status_code, reason
        else if result.headers and conf.send_headers_upstream then
            for _, name in ipairs(conf.send_headers_upstream) do
                local value = result.headers[name]
                if value then
                    core.request.set_header(ctx, name, value)
                end
            end
            end
        end
    end


    return _M

  helper.lua: |-
    --
    -- Licensed to the Apache Software Foundation (ASF) under one or more
    -- contributor license agreements.  See the NOTICE file distributed with
    -- this work for additional information regarding copyright ownership.
    -- The ASF licenses this file to You under the Apache License, Version 2.0
    -- (the "License"); you may not use this file except in compliance with
    -- the License.  You may obtain a copy of the License at
    --
    --     http://www.apache.org/licenses/LICENSE-2.0
    --
    -- Unless required by applicable law or agreed to in writing, software
    -- distributed under the License is distributed on an "AS IS" BASIS,
    -- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    -- See the License for the specific language governing permissions and
    -- limitations under the License.
    --
    
    local core        = require("apisix.core")
    local get_service = require("apisix.http.service").get
    local ngx_time    = ngx.time
    
    local _M = {}
    
    
    -- build a table of Nginx variables with some generality
    -- between http subsystem and stream subsystem
    local function build_var(conf, ctx)
      return {
        server_addr = ctx.var.server_addr,
        server_port = ctx.var.server_port,
        remote_addr = ctx.var.remote_addr,
        remote_port = ctx.var.remote_port,
        timestamp   = ngx_time(),
      }
    end
    
    
    local function build_http_request(conf, ctx)
    
      local http = {
        scheme  = core.request.get_scheme(ctx),
        method  = core.request.get_method(),
        host    = core.request.get_host(ctx),
        port    = core.request.get_port(ctx),
        path    = ctx.var.uri,
        headers = core.request.headers(ctx),
        query   = core.request.get_uri_args(ctx),
      }
    
      if conf.with_body then
        http.body = core.json.decode(core.request.get_body())
      end
      
      return http
    end
    
    
    local function build_http_route(conf, ctx, remove_upstream)
      local route = core.table.deepcopy(ctx.matched_route).value
    
      if remove_upstream and route and route.upstream then
        -- unimportant to send upstream info to OPA
        route.upstream = nil
      end
    
      return route
    end
    
    
    local function build_http_service(conf, ctx)
      local service_id = ctx.service_id
      
      -- possible that there is no service bound to the route
      if service_id then
        local service = core.table.clone(get_service(service_id)).value
      
        if service then
          if service.upstream then
            service.upstream = nil
          end
          return service
        end
      end
      
      return nil
    end
    
    
    local function build_http_consumer(conf, ctx)
      -- possible that there is no consumer bound to the route
      if ctx.consumer then
        return core.table.clone(ctx.consumer)
      end
      
      return nil
    end
    
    
    function _M.build_opa_input(conf, ctx, subsystem)
      local data = {
        type    = subsystem,
        request = build_http_request(conf, ctx),
        var     = build_var(conf, ctx)
      }
      
      if conf.with_route then
        data.route = build_http_route(conf, ctx, true)
      end
      
      if conf.with_consumer then
        data.consumer = build_http_consumer(conf, ctx)
      end
      
      if conf.with_service then
        data.service = build_http_service(conf, ctx)
      end
      
      return {
        input = data,
      }
    end
    
    
    return _M
---
# Source: data-space-connector/templates/realm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-realm-realm
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  test-realm-realm.json: |-
    {
      "id": "test-realm",
      "realm": "test-realm",
      "displayName": "Keycloak",
      "displayNameHtml": "<div class=\"kc-logo-text\"><span>Keycloak</span></div>",
      "enabled": true,
      "attributes": {
        "frontendUrl": "",
        "issuerDid": "${DID}"
      },
      "sslRequired": "none",
      "roles": {
        "realm": [
          {
            "name": "user",
            "description": "User privileges",
            "composite": false,
            "clientRole": false,
            "containerId": "dome",
            "attributes": {}
          }
        ],
        "client": {
          
          "${DID}": [
            {
              "name": "ADMIN",
              "description": "Is allowed to do everything",
              "clientRole": true
            }
          ]
          
        }
      },
      "groups": [
      ],
      "users": [
        
        {
          "username": "admin-user",
          "enabled": true,
          "email": "admin@provider.org",
          "firstName": "Test",
          "lastName": "Admin",
          "credentials": [
            {
              "type": "password",
              "value": "test"
            }
          ],
          "clientRoles": {
            "${DID}": [
              "ADMIN"
            ],
            "account": [
              "view-profile",
              "manage-account"
            ]
          },
          "groups": [
          ]
        }
        
      ],
      "clients": [
        
        {
          "clientId": "${DID}",
          "enabled": true,
          "description": "Client to manage itself",
          "surrogateAuthRequired": false,
          "alwaysDisplayInConsole": false,
          "clientAuthenticatorType": "client-secret",
          "defaultRoles": [],
          "redirectUris": [],
          "webOrigins": [],
          "notBefore": 0,
          "bearerOnly": false,
          "consentRequired": false,
          "standardFlowEnabled": true,
          "implicitFlowEnabled": false,
          "directAccessGrantsEnabled": false,
          "serviceAccountsEnabled": false,
          "publicClient": false,
          "frontchannelLogout": false,
          "protocol": "oid4vc",
          "attributes": {
            "client.secret.creation.time": "1675260539",
            "vc.natural-person.format": "jwt_vc",
            "vc.natural-person.scope": "NaturalPersonCredential",
            "vc.verifiable-credential.format": "jwt_vc",
            "vc.verifiable-credential.scope": "VerifiableCredential"
          },
          "protocolMappers": [
            {
              "name": "target-role-mapper",
              "protocol": "oid4vc",
              "protocolMapper": "oid4vc-target-role-mapper",
              "config": {
                "subjectProperty": "roles",
                "clientId": "${DID}",
                "supportedCredentialTypes": "NaturalPersonCredential"
              }
            },
            {
              "name": "target-vc-role-mapper",
              "protocol": "oid4vc",
              "protocolMapper": "oid4vc-target-role-mapper",
              "config": {
                "subjectProperty": "roles",
                "clientId": "${DID}",
                "supportedCredentialTypes": "VerifiableCredential"
              }
            },
            {
              "name": "context-mapper",
              "protocol": "oid4vc",
              "protocolMapper": "oid4vc-context-mapper",
              "config": {
                "context": "https://www.w3.org/2018/credentials/v1",
                "supportedCredentialTypes": "VerifiableCredential,NaturalPersonCredential"
              }
            },
            {
              "name": "email-mapper",
              "protocol": "oid4vc",
              "protocolMapper": "oid4vc-user-attribute-mapper",
              "config": {
                "subjectProperty": "email",
                "userAttribute": "email",
                "supportedCredentialTypes": "NaturalPersonCredential"
              }
            }
          ],
          "authenticationFlowBindingOverrides": {},
          "fullScopeAllowed": true,
          "nodeReRegistrationTimeout": -1,
          "defaultClientScopes": [],
          "optionalClientScopes": []
        }
        
      ],
      "clientScopes": [
        {
          "name": "roles",
          "description": "OpenID Connect scope for add user roles to the access token",
          "protocol": "openid-connect",
          "attributes": {
            "include.in.token.scope": "false",
            "display.on.consent.screen": "true",
            "consent.screen.text": "${rolesScopeConsentText}"
          },
          "protocolMappers": [
            {
              "name": "audience resolve",
              "protocol": "openid-connect",
              "protocolMapper": "oidc-audience-resolve-mapper",
              "consentRequired": false,
              "config": {
                "introspection.token.claim": "true",
                "access.token.claim": "true"
              }
            },
            {
              "name": "realm roles",
              "protocol": "openid-connect",
              "protocolMapper": "oidc-usermodel-realm-role-mapper",
              "consentRequired": false,
              "config": {
                "introspection.token.claim": "true",
                "multivalued": "true",
                "user.attribute": "foo",
                "access.token.claim": "true",
                "claim.name": "realm_access.roles",
                "jsonType.label": "String"
              }
            },
            {
              "name": "client roles",
              "protocol": "openid-connect",
              "protocolMapper": "oidc-usermodel-client-role-mapper",
              "consentRequired": false,
              "config": {
                "introspection.token.claim": "true",
                "multivalued": "true",
                "user.attribute": "foo",
                "access.token.claim": "true",
                "claim.name": "resource_access.${client_id}.roles",
                "jsonType.label": "String"
              }
            }
          ]
        }
      ],
      "defaultDefaultClientScopes": [
      ],
      "defaultOptionalClientScopes": [
      ],
      "components": {
        "org.keycloak.protocol.oid4vc.issuance.signing.VerifiableCredentialsSigningService": [
          {
            "id": "jwt-signing",
            "name": "jwt-signing-service",
            "providerId": "jwt_vc",
            "subComponents": {},
            "config": {
              "keyId": [
                "${DID}"
              ],
              "algorithmType": [
                "ES256"
              ],
              "issuerDid": [
                "${DID}"
              ],
              "tokenType": [
                "JWT"
              ]
            }
          }
        ],
        "org.keycloak.keys.KeyProvider": [
          {
            "id": "a4589e8f-7f82-4345-b2ea-ccc9d4366600",
            "name": "test-key",
            "providerId": "java-keystore",
            "subComponents": {},
            "config": {
              "keystore": [ "/did-material/cert.pfx" ],
              "keystorePassword": [ "${STORE_PASS}" ],
              "keyAlias": [ "didPrivateKey" ],
              "keyPassword": [ "${STORE_PASS}" ],
              "kid": [ "${DID}"],
              "active": [
                "true"
              ],
              "priority": [
                "0"
              ],
              "enabled": [
                "true"
              ],
              "algorithm": [
                "ES256"
              ]
            }
          }
        ]
      }
    }
---
# Source: data-space-connector/templates/tmf-registration-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tmf-api-registration
  namespace: "provider"
  labels:
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  init.sh: |-
    # credentials config service registration
    curl -X 'POST' \
      'http://credentials-config-service:8080/service' \
      -H 'accept: */*' \
      -H 'Content-Type: application/json' \
      -d '{
      "id": "tmf-api",
      "defaultOidcScope": "default","oidcScopes": {
        "default": [
          {
            "type": "UserCredential",
            "trustedParticipantsLists": [
              "http://tir.trust-anchor.svc.cluster.local:8080"
            ],
            "trustedIssuersLists": [
              "http://trusted-issuers-list:8080"
            ]
          }
        ]
      }
    }'
---
# Source: data-space-connector/templates/tpp-policy.yaml
apiVersion: v1
kind: ConfigMap
metadata: 
  name: tpp-policy
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
data:
  tpp.rego: |-
    package tpp
    
    import rego.v1
    import data.policy.main

    default allow := false

    host = ""
    path = "/transfers"
    url = sprintf("%v/%v/%v", [host, path, input.request.headers["transferid"]])

    response := http.send({"method": "get", "url": url})
    allow if response.body["dspace:state"] == "dspace:STARTED"
---
# Source: data-space-connector/charts/apisix/templates/control-plane/clusterroles.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: provider-dsc-apisix-provider-control-plane
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch"]
---
# Source: data-space-connector/charts/apisix/templates/data-plane/clusterroles.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: provider-dsc-apisix-provider-data-plane
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch"]
---
# Source: data-space-connector/charts/apisix/templates/control-plane/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: provider-dsc-apisix-provider-control-plane
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: provider-dsc-apisix-provider-control-plane
subjects:
  - kind: ServiceAccount
    name: provider-dsc-apisix-control-plane
    namespace: "provider"
---
# Source: data-space-connector/charts/apisix/templates/data-plane/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: provider-dsc-apisix-provider-data-plane
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: provider-dsc-apisix-provider-data-plane
subjects:
  - kind: ServiceAccount
    name: provider-dsc-apisix-data-plane
    namespace: "provider"
---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-etcd-headless
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
    app.kubernetes.io/component: etcd
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: etcd
    app.kubernetes.io/component: etcd
---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-etcd
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
    app.kubernetes.io/component: etcd
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: etcd
    app.kubernetes.io/component: etcd
---
# Source: data-space-connector/charts/apisix/templates/control-plane/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-apisix-control-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: https-admin-api
      port: 9180
      protocol: TCP
      nodePort: null
      targetPort: https-admin-api
    - name: https-cfg-srv
      port: 9280
      protocol: TCP
      nodePort: null
      targetPort: https-cfg-srv
    - name: rustapitest-port
      port: 8085
      targetPort: 8085
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: apisix
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
---
# Source: data-space-connector/charts/apisix/templates/data-plane/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-apisix-data-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
spec:
  type: LoadBalancer
  sessionAffinity: None
  externalTrafficPolicy: "Cluster"
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: apisix
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
---
# Source: data-space-connector/charts/contract-management/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: contract-management
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: contract-management
    helm.sh/chart: contract-management-1.0.2
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: contract-management
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/credentials-config-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: credentials-config-service
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: credentials-config-service
    helm.sh/chart: credentials-config-service-1.0.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: credentials-config-service
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/mysql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentication-mysql-headless
  namespace: "provider"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.4
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.0.31"
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mysql
      port: 3306
      targetPort: mysql
  selector: 
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/component: primary
---
# Source: data-space-connector/charts/mysql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: authentication-mysql
  namespace: "provider"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.4
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.0.31"
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
      nodePort: null
  selector: 
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/component: primary
---
# Source: data-space-connector/charts/odrl-pap/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: odrl-pap
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: odrl-pap
    helm.sh/chart: odrl-pap-0.2.2
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: odrl-pap
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/postgis/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: data-service-postgis-hl
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: data-service-postgis
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgis-13.1.5
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: data-service-postgis
    app.kubernetes.io/component: primary
---
# Source: data-space-connector/charts/postgis/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: data-service-postgis
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: data-service-postgis
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgis-13.1.5
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: data-service-postgis
    app.kubernetes.io/component: primary
---
# Source: data-space-connector/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgresql-hl
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgresql-13.1.5
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: data-space-connector/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgresql
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgresql-13.1.5
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: data-space-connector/charts/scorpio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: data-service-scorpio
  namespace: "provider"
  labels:
    helm.sh/chart: scorpio-0.4.7
    app.kubernetes.io/name: scorpio
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9090
      targetPort: 9090
      protocol: TCP
      name: "9090"
  selector:
    app.kubernetes.io/name: scorpio
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/envoy-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: tm-forum-api
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-envoy
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-party-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-party-catalog
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-customer-bill-management
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-customer-bill-management
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-customer-management
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-customer-management
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-product-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-product-catalog
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-product-inventory
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-product-inventory
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-product-ordering-management
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-product-ordering-management
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-resource-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-resource-catalog
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-resource-function-activation
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-resource-function-activation
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-resource-inventory
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-resource-inventory
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-service-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-service-catalog
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/tm-forum-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: provider-dsc-tm-forum-api-agreement
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: tm-forum-api-agreement
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/trusted-issuers-list/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: trusted-issuers-list
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: trusted-issuers-list
    helm.sh/chart: trusted-issuers-list-0.7.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: trusted-issuers-list
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/vcverifier/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: verifier
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: vcverifier
    helm.sh/chart: vcverifier-2.13.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 3000
      targetPort: backend
      protocol: TCP
  selector:
    app.kubernetes.io/name: vcverifier
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/templates/dsconfig-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dsconfig
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3002
      targetPort: 3000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: dsconfig
    app.kubernetes.io/instance: provider-dsc
---
# Source: data-space-connector/charts/apisix/templates/control-plane/dep-ds.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-apisix-control-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: control-plane
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: control-plane
  template:
    metadata:
      annotations:
        checksum/config: 7ca210e3f1c314e5b8bf02d088e6069232a3199c1a8bcbd72bce6779243431a8
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/secret: 56ab3eec8cf90473af4897dd67e5328169543dd33fd850a706e4fd73e4d8ece2
      labels:
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: apisix
        app.kubernetes.io/version: 3.10.0
        helm.sh/chart: apisix-3.5.1
        app.kubernetes.io/part-of: apisix
        app.kubernetes.io/component: control-plane
    spec:
      serviceAccountName: provider-dsc-apisix-control-plane
      
      automountServiceAccountToken: true
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: provider-dsc
                    app.kubernetes.io/name: apisix
                    app.kubernetes.io/component: control-plane
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:12-debian-12-r30
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://provider-dsc-etcd:2379"
              )
        
              check_etcd() {
                  local curl_options=()
        
                  local -r etcd_host="${1:-?missing etcd}"
                  if curl "${curl_options[@]}" --max-time 5 "${etcd_host}/version" | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the APISIX configuration files, as well
        # as preparing the Nginx server. We need to use a volume because we're working with
        # ReadOnlyRootFilesystem
        - name: prepare-apisix
          image: docker.io/bitnami/apisix:3.10.0-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              cp -R /opt/bitnami/apisix/conf /usr/local/apisix
              ln -sf /opt/bitnami/apisix/deps /usr/local/apisix
              ln -sf /opt/bitnami/apisix/openresty/luajit/share/lua/*/apisix /usr/local/apisix
              mkdir -p /usr/local/apisix/logs
              # Build final config.yaml with the sections of the different files
              find /bitnami/apisix/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' > /usr/local/apisix/conf/pre-render-config.yaml
              render-template /usr/local/apisix/conf/pre-render-config.yaml > /usr/local/apisix/conf/config.yaml
              rm /usr/local/apisix/conf/pre-render-config.yaml
              chmod 644 /usr/local/apisix/conf/config.yaml
              apisix init
              apisix init_etcd
              # The path is hardcoded in the conf so we need to copy them to the server folder
              cp /bitnami/certs/tls.crt /usr/local/apisix/conf/cert/ssl_PLACE_HOLDER.crt
              cp /bitnami/certs/tls.key /usr/local/apisix/conf/cert/ssl_PLACE_HOLDER.key
          env:
            - name: BITNAMI_DEBUG
              value: "true"
            - name: APISIX_ADMIN_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: provider-dsc-apisix-control-plane-api-token
                  key: admin-token
            - name: APISIX_VIEWER_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: provider-dsc-apisix-control-plane-api-token
                  key: viewer-token
          envFrom:
          volumeMounts:
            - name: empty-dir
              mountPath: /usr/local/apisix
              subPath: app-tmp-dir
            - name: config
              mountPath: /bitnami/apisix/conf/00_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: certs
              mountPath: /bitnami/certs
      containers:
        - name: apisix
          image: docker.io/bitnami/apisix:3.10.0-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - openresty
          args:
            - -p
            - /usr/local/apisix
            - -g
            - "daemon off;"
          env:
          envFrom:
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          ports:
            - name: https-admin-api
              containerPort: 9180
            - name: https-cfg-srv
              containerPort: 9280
            - name: http-control
              containerPort: 9090
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-control
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /v1/healthcheck
              port: http-control
          volumeMounts:
            - name: empty-dir
              mountPath: /usr/local/apisix
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: certs
              mountPath: /bitnami/certs
        - command:
          - /bin/sh
          - -c
          - ' apk add --no-cache curl jq; payload=''{"id":"rustapitest-service","defaultOidcScope":"default","oidcScopes":{"default":[{"type":"UserCredential","trustedParticipantsLists":["http://tir.trust-anchor.svc.cluster.local:8080"],"trustedIssuersLists":["http://trusted-issuers-list:8080"]}],"operator":[{"type":"OperatorCredential","trustedParticipantsLists":["http://tir.trust-anchor.svc.cluster.local:8080"],"trustedIssuersLists":["http://trusted-issuers-list:8080"]}]}}'';
            status_code=0; response=''''; echo "Registering service with credentials-config-service";
            while [ $status_code -ne 200 ] || [ -z $response ]; do echo "Sending request";
            response=$(curl -X POST http://credentials-config-service:8080/service -H "Content-Type:
            application/json" -d "$payload"); status_code=$(echo $response | jq -r ''.status'');
            echo "Response status code: $status_code"; echo "Response body: $response"; done;
            echo "Service registered successfully"; sleep infinity; '
          image: alpine:latest
          imagePullPolicy: IfNotPresent
          name: register-ccs
        - command:
          - /bin/sh
          - -c
          - apk add curl; while true; do echo "Hello!"; sleep 30; done
          image: alpine:latest
          imagePullPolicy: IfNotPresent
          name: test-maintenance
        - image: cristianmartella/rustapitest:0.1.0
          imagePullPolicy: IfNotPresent
          name: rustapitest
          ports:
          - containerPort: 8085
            name: http
            protocol: TCP
      volumes:
        - name: config
          configMap:
            name: provider-dsc-apisix-control-plane-default
        - name: certs
          secret:
            secretName: provider-dsc-apisix-control-plane-tls
        - name: empty-dir
          emptyDir: {}
---
# Source: data-space-connector/charts/apisix/templates/data-plane/dep-ds.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-apisix-data-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: apisix
      app.kubernetes.io/part-of: apisix
      app.kubernetes.io/component: data-plane
  template:
    metadata:
      annotations:
        checksum/config: 75f92e12bbb04c0a38b47cf7916d21a30c0bfc50261979b4f23e6d5bbea57181
        checksum/config-extra: d87cc20e5369da6f6cd046510a46f497858fc17a4744fcffaa868be16999bc94
        checksum/secret: d410490e3ea3db692317a44452acab5372929d06d01ef41d73349a18fd4a009a
      labels:
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: apisix
        app.kubernetes.io/version: 3.10.0
        helm.sh/chart: apisix-3.5.1
        app.kubernetes.io/part-of: apisix
        app.kubernetes.io/component: data-plane
    spec:
      serviceAccountName: provider-dsc-apisix-data-plane
      
      automountServiceAccountToken: true
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: provider-dsc
                    app.kubernetes.io/name: apisix
                    app.kubernetes.io/component: data-plane
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: wait-for-control-plane
          image: docker.io/bitnami/os-shell:12-debian-12-r30
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              host="https://provider-dsc-apisix-control-plane:9180"
        
              check_control_plane() {
                  if curl --max-time 5 -k -I "$host"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              echo "Checking connection to $host"
              if retry_while "check_control_plane"; then
                  echo "Connected to $host"
              else
                  echo "Error connecting to $host"
                  exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the APISIX configuration files, as well
        # as preparing the Nginx server. We need to use a volume because we're working with
        # ReadOnlyRootFilesystem
        - name: prepare-apisix
          image: docker.io/bitnami/apisix:3.10.0-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              cp -R /opt/bitnami/apisix/conf /usr/local/apisix
              ln -sf /opt/bitnami/apisix/deps /usr/local/apisix
              ln -sf /opt/bitnami/apisix/openresty/luajit/share/lua/*/apisix /usr/local/apisix
              mkdir -p /usr/local/apisix/logs
              # Build final config.yaml with the sections of the different files
              find /bitnami/apisix/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' > /usr/local/apisix/conf/pre-render-config.yaml
              render-template /usr/local/apisix/conf/pre-render-config.yaml > /usr/local/apisix/conf/config.yaml
              rm /usr/local/apisix/conf/pre-render-config.yaml
              chmod 644 /usr/local/apisix/conf/config.yaml
              apisix init
              # The path is hardcoded in the conf so we need to copy them to the server folder
              cp /bitnami/certs/tls.crt /usr/local/apisix/conf/cert/ssl_PLACE_HOLDER.crt
              cp /bitnami/certs/tls.key /usr/local/apisix/conf/cert/ssl_PLACE_HOLDER.key
          env:
            - name: BITNAMI_DEBUG
              value: "true"
            - name: APISIX_ADMIN_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: provider-dsc-apisix-control-plane-api-token
                  key: admin-token
            - name: APISIX_VIEWER_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: provider-dsc-apisix-control-plane-api-token
                  key: viewer-token
          envFrom:
          volumeMounts:
            - name: empty-dir
              mountPath: /usr/local/apisix
              subPath: app-tmp-dir
            - name: config
              mountPath: /bitnami/apisix/conf/00_default
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: extra-config
              mountPath: /bitnami/apisix/conf/01_extra
            - name: certs
              mountPath: /bitnami/certs
      containers:
        - name: apisix
          image: docker.io/bitnami/apisix:3.10.0-debian-12-r2
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - openresty
          args:
            - -p
            - /usr/local/apisix
            - -g
            - "daemon off;"
          env:
          envFrom:
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          ports:
            - name: http
              containerPort: 9080
            - name: https
              containerPort: 9443
            - name: http-control
              containerPort: 9090
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http-control
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /v1/healthcheck
              port: http-control
          volumeMounts:
            - name: empty-dir
              mountPath: /usr/local/apisix
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: certs
              mountPath: /bitnami/certs
            - name: control-plane-certs
              mountPath: /etc/ssl/certs/ca.crt
              subPath: ca.crt
            - mountPath: /usr/local/apisix/conf/apisix.yaml
              name: routes
              subPath: apisix.yaml
            - mountPath: /usr/local/apisix/apisix/plugins/opa/helper.lua
              name: opa-lua
              subPath: helper.lua
            - mountPath: /usr/local/apisix/apisix/plugins/opa.lua
              name: opa-lua
              subPath: opa.lua
        - args:
          - run
          - --ignore=.*
          - --server
          - -l
          - debug
          - -c
          - /config/opa.yaml
          - --addr
          - 0.0.0.0:8181
          - /tpp/tpp.rego
          image: openpolicyagent/opa:0.64.1
          imagePullPolicy: IfNotPresent
          name: open-policy-agent
          ports:
          - containerPort: 8181
            name: http
            protocol: TCP
          volumeMounts:
          - mountPath: /config
            name: opa-config
          - mountPath: /tpp
            name: tpp-policy
      volumes:
        - name: config
          configMap:
            name: provider-dsc-apisix-data-plane-default
        - name: extra-config
          configMap:
            name: provider-dsc-apisix-data-plane-extra
        - name: certs
          secret:
            secretName: provider-dsc-apisix-data-plane-tls
        - name: empty-dir
          emptyDir: {}
        - name: control-plane-certs
          secret:
            secretName: provider-dsc-apisix-control-plane-tls
        - configMap:
            name: apisix-routes
          name: routes
        - configMap:
            name: opa-config
          name: opa-config
        - configMap:
            name: opa-lua
          name: opa-lua
        - configMap:
            name: tpp-policy
          name: tpp-policy
---
# Source: data-space-connector/charts/contract-management/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: contract-management
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: contract-management
    helm.sh/chart: contract-management-1.0.2
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: contract-management
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: contract-management
        helm.sh/chart: contract-management-1.0.2
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/version: "2.0.0"
        app.kubernetes.io/managed-by: Helm
    spec: 
      serviceAccountName: default
      containers:
        - name: contract-management
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/contract-management:2.0.1"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: http-health
            initialDelaySeconds: 31
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: ENDPOINTS_METRICS_ENABLED
              value: "true"
            - name: MICRONAUT_CONFIG_FILES
              value: "/application.yaml"
          volumeMounts:
            - name: application-yaml
              mountPath: /application.yaml
              subPath: application.yaml
          resources:
            null
      volumes:
        - name: application-yaml
          configMap:
            name: contract-management
            items:
              - key: application.yaml
                path: application.yaml
---
# Source: data-space-connector/charts/credentials-config-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: credentials-config-service
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: credentials-config-service
    helm.sh/chart: credentials-config-service-1.0.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: credentials-config-service
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: credentials-config-service
        helm.sh/chart: credentials-config-service-1.0.0
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/version: "0.0.1"
        app.kubernetes.io/managed-by: Helm
    spec: 
      serviceAccountName: default
      containers:
        - name: credentials-config-service
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/credentials-config-service:2.0.0"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: http-health
            initialDelaySeconds: 31
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: DATASOURCES_DEFAULT_URL
              value: "jdbc:mysql://authentication-mysql:3306/ccsdb"
            - name: DATASOURCES_DEFAULT_DRIVER_CLASS_NAME
              value: "com.mysql.cj.jdbc.Driver"
            - name: DATASOURCES_DEFAULT_USERNAME
              value: "root"
            - name: DATASOURCES_DEFAULT_DIALECT
              value: "MYSQL"
            - name: DATASOURCES_DEFAULT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: authentication-database-secret
                  key: mysql-root-password
          resources:
            null
---
# Source: data-space-connector/charts/odrl-pap/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: odrl-pap
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: odrl-pap
    helm.sh/chart: odrl-pap-0.2.2
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: odrl-pap
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: odrl-pap
        helm.sh/chart: odrl-pap-0.2.2
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/version: "0.1.4"
        app.kubernetes.io/managed-by: Helm
    spec: 
      serviceAccountName: default
      containers:
        - name: odrl-pap
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/odrl-pap:0.1.5"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /q/health/live
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /q/health/ready
              port: http-health
            initialDelaySeconds: 31
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: QUARKUS_HTTP_PORT
              value: "8080"
            - name: QUARKUS_MANAGEMENT_PORT
              value: "9090"
            - name: QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED
              value: "true"
            - name: QUARKUS_DATASOURCE_USERNAME
              value: "postgres"
            - name: QUARKUS_DATASOURCE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: database-secret
                  key: postgres-admin-password
            - name: QUARKUS_DATASOURCE_JDBC_URL
              value: "jdbc:postgresql://postgresql:5432/pap"
          volumeMounts:
          resources:
            null
      volumes:
---
# Source: data-space-connector/charts/scorpio/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-service-scorpio
  namespace: "provider"
  labels:
    helm.sh/chart: scorpio-0.4.7
    app.kubernetes.io/name: scorpio
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: scorpio
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: scorpio
        app.kubernetes.io/instance: provider-dsc
    spec:
      serviceAccountName: default
      securityContext:
        {}
      initContainers:
        - command:
          - /bin/sh
          - -c
          - /bin/init.sh
          image: quay.io/curl/curl:8.1.2
          name: register-credential-config
          volumeMounts:
          - mountPath: /bin/init.sh
            name: scorpio-registration
            subPath: init.sh
      containers:
        - name: scorpio
          securityContext:
            {}
          image: "scorpiobroker/all-in-one-runner:java-4.1.11"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /q/health
              port: 9090
            initialDelaySeconds: 40
            periodSeconds: 10
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /q/health
              port: 9090
            initialDelaySeconds: 40
            periodSeconds: 10
            failureThreshold: 6
    
          env:
            - name: DBHOST
              value: data-service-postgis
            - name: DBPASS
              valueFrom:
                secretKeyRef:
                  name: data-service-secret
                  key: postgres-admin-password
            - name: DBUSER
              value: postgres
            - name: POSTGRES_DB
              value: data-service-postgis
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: data-service-secret
                  key: postgres-admin-password
            - name: POSTGRES_USER
              value: postgres  
            - name: KAFKA_ADVERTISED_HOST_NAME
              value: kafka
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: zookeeper:2181
            - name: KAFKA_ADVERTISED_PORT
              value: "9092"
            - name: QUARKUS_LOG_LEVEL
              value: "INFO"
          resources:
            {}
      volumes:
      - configMap:
          defaultMode: 493
          name: scorpio-registration
        name: scorpio-registration
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-party-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-party-catalog
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-party-catalog
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: party-catalog
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-party-catalog:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/party/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-customer-bill-management
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-customer-bill-management
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-customer-bill-management
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: customer-bill-management
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-customer-bill-management:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/customerBillManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-customer-management
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-customer-management
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-customer-management
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: customer-management
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-customer-management:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/customerManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-product-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-product-catalog
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-product-catalog
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: product-catalog
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-product-catalog:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/productCatalogManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-product-inventory
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-product-inventory
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-product-inventory
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: product-inventory
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-product-inventory:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/productInventory/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-product-ordering-management
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-product-ordering-management
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-product-ordering-management
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: product-ordering-management
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-product-ordering-management:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/productOrderingManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-resource-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-resource-catalog
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-resource-catalog
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: resource-catalog
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-resource-catalog:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/resourceCatalog/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-resource-function-activation
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-resource-function-activation
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-resource-function-activation
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: resource-function-activation
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-resource-function-activation:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/resourceFunctionActivation/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-resource-inventory
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-resource-inventory
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-resource-inventory
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: resource-inventory
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-resource-inventory:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/resourceInventoryManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-service-catalog
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-service-catalog
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-service-catalog
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: service-catalog
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-service-catalog:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/serviceCatalogManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-agreement
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-agreement
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-agreement
        app.kubernetes.io/instance: provider-dsc
        
        helm.sh/chart: tm-forum-api-0.10.10
        app.kubernetes.io/version: "0.13.2"
        app.kubernetes.io/managed-by: Helm
      annotations: 
        prometheus.io/scrape: "true"
        prometheus.io/path: /prometheus
        prometheus.io/port: "9090"
    spec: 
      serviceAccountName: default
      containers:
        - name: agreement
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/tmforum-agreement:0.30.7"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: ENDPOINTS_HEALTH_ENABLED
              value: "true"
            - name: LOGGER_LEVELS_ROOT
              value: "INFO"
            - name: MICRONAUT_HTTP_SERVICES_READ_TIMEOUT
              value: "30s"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_PATH
              value: "ngsi-ld/v1"
            - name: MICRONAUT_HTTP_SERVICES_NGSI_URL
              value: "http://data-service-scorpio:9090"
            - name: GENERAL_BASEPATH
              value: "/tmf-api/agreementManagement/v4"
            - name: GENERAL_CONTEXT_URL
              value: "https://uri.etsi.org/ngsi-ld/v1/ngsi-ld-core-context.jsonld"
            - name: GENERAL_SERVER_HOST
              value: "http://tmf-api:8080"
            - name: MICRONAUT_ENVIRONMENTS
              value: in-memory
            - name: MICRONAUT_CACHES_ENTITIES_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_WRITE
              value: "2s"
            - name: MICRONAUT_CACHES_ENTITIES_EXPIRE_AFTER_ACCESS
              value: "2s"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_MAXIMUM_SIZE
              value: "1000"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_WRITE
              value: "14d"
            - name: MICRONAUT_CACHES_SUBSCRIPTIONS_EXPIRE_AFTER_ACCESS
              value: "14d"
            - name: API_EXTENSION_ENABLED
              value: "true"
---
# Source: data-space-connector/charts/tm-forum-api/templates/envoy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-dsc-tm-forum-api-envoy
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: tm-forum-api-envoy
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tm-forum-api-envoy
        app.kubernetes.io/instance: provider-dsc
    spec: 
      containers:
        - name: envoy-proxy
          imagePullPolicy:  IfNotPresent
          image: "envoyproxy/envoy:distroless-v1.27-latest"
          args: 
            - -c /config/envoy-config.yaml
            - -l debug
          ports:
            - name: http
              containerPort: 10000
              protocol: TCP    
          volumeMounts:
            - name: config
              mountPath: /config
      volumes:
        - name: config
          configMap:
            name: provider-dsc-tm-forum-api-envoy
---
# Source: data-space-connector/charts/trusted-issuers-list/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trusted-issuers-list
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: trusted-issuers-list
    helm.sh/chart: trusted-issuers-list-0.7.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: trusted-issuers-list
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: trusted-issuers-list
        helm.sh/chart: trusted-issuers-list-0.7.0
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/version: "0.0.2"
        app.kubernetes.io/managed-by: Helm
    spec: 
      serviceAccountName: default
      containers:
        - name: trusted-issuers-list
          imagePullPolicy: IfNotPresent
          image: "quay.io/fiware/trusted-issuers-list:0.2.1"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: http-health
              containerPort: 9090
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http-health
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: http-health
            initialDelaySeconds: 31
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          env:
            - name: ENDPOINTS_ALL_PORT
              value: "9090"
            - name: MICRONAUT_SERVER_PORT
              value: "8080"
            - name: MICRONAUT_METRICS_ENABLED
              value: "true"
            - name: DATASOURCES_DEFAULT_URL
              value: "jdbc:mysql://authentication-mysql:3306/tildb"
            - name: DATASOURCES_DEFAULT_DRIVER_CLASS_NAME
              value: "com.mysql.cj.jdbc.Driver"
            - name: DATASOURCES_DEFAULT_USERNAME
              value: "root"
            - name: DATASOURCES_DEFAULT_DIALECT
              value: "MYSQL"
            - name: DATASOURCES_DEFAULT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: authentication-database-secret
                  key: mysql-root-password
          resources:
            null
---
# Source: data-space-connector/charts/vcverifier/templates/deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: verifier
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: vcverifier
    helm.sh/chart: vcverifier-2.13.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: vcverifier
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: vcverifier
        helm.sh/chart: vcverifier-2.13.0
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/version: "2.0.1"
        app.kubernetes.io/managed-by: Helm
    spec: 
      serviceAccountName: default
      containers:
        - name: vcverifier
          image: "quay.io/fiware/vcverifier:4.8.0"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3000
              name: backend
          env:
            - name: CONFIG_FILE
              value: /configs/server.yaml
          livenessProbe:
            httpGet:
              path: /health
              port: backend
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: backend
            initialDelaySeconds: 4
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            null
          ## workaround for missing db support
          volumeMounts:
            - mountPath: /configs/
              name: config-volume
      volumes:
      - name: config-volume
        configMap:
          name: verifier
---
# Source: data-space-connector/templates/dsconfig-deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: dsconfig
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: dsconfig
      app.kubernetes.io/instance: provider-dsc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dsconfig
        app.kubernetes.io/instance: provider-dsc
    spec:
      serviceAccountName: default
      containers:
        - name: dsconfig-static
          imagePullPolicy: Always
          image: "lipanski/docker-static-website:2.1.0"
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          volumeMounts:
            - name: dsconfig-json
              mountPath: /home/static/.well-known/data-space-configuration
      volumes:
        - name: dsconfig-json
          configMap:
            name: dsconfig
---
# Source: data-space-connector/charts/apisix/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: provider-dsc-etcd
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.16
    helm.sh/chart: etcd-10.2.18
    app.kubernetes.io/component: etcd
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
  serviceName: provider-dsc-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: etcd
        app.kubernetes.io/version: 3.5.16
        helm.sh/chart: etcd-10.2.18
        app.kubernetes.io/component: etcd
      annotations:
        checksum/token-secret: 367b90000e6465f00a775dfbd6852ac9efa3486602e6b3746276089a1d7f7d1b
    spec:
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: provider-dsc
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/component: etcd
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: "provider-dsc-etcd"
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.16-debian-12-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_STS_NAME
              value: "provider-dsc-etcd"
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: "jwt,priv-key=/opt/bitnami/etcd/certs/token/jwt-token.pem,sign-method=RS256,ttl=10m"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).provider-dsc-etcd-headless.provider.svc.cluster.local:2379,http://provider-dsc-etcd.provider.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).provider-dsc-etcd-headless.provider.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_INITIAL_CLUSTER_TOKEN
              value: "etcd-cluster-k8s"
            - name: ETCD_INITIAL_CLUSTER_STATE
              value: "new"
            - name: ETCD_INITIAL_CLUSTER
              value: "provider-dsc-etcd-0=http://provider-dsc-etcd-0.provider-dsc-etcd-headless.provider.svc.cluster.local:2380,provider-dsc-etcd-1=http://provider-dsc-etcd-1.provider-dsc-etcd-headless.provider.svc.cluster.local:2380,provider-dsc-etcd-2=http://provider-dsc-etcd-2.provider-dsc-etcd-headless.provider.svc.cluster.local:2380"
            - name: ETCD_CLUSTER_DOMAIN
              value: "provider-dsc-etcd-headless.provider.svc.cluster.local"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            httpGet:
              port: 2379 
              path: /livez
              scheme: "HTTP"
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - /opt/bitnami/scripts/etcd/prestop.sh
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 2Gi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /opt/bitnami/etcd/conf/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: data
              mountPath: /bitnami/etcd
            - name: etcd-jwt-token
              mountPath: /opt/bitnami/etcd/certs/token/
              readOnly: true
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: etcd-jwt-token
          secret:
            secretName: provider-dsc-etcd-jwt-token
            defaultMode: 256
        - name: data
          emptyDir: {}
---
# Source: data-space-connector/charts/mysql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: authentication-mysql
  namespace: "provider"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.4
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.0.31"
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  podManagementPolicy: ""
  selector:
    matchLabels: 
      app.kubernetes.io/name: mysql
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/component: primary
  serviceName: authentication-mysql
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configuration: 4373b1182fec6480d2ef8456c6f970ace28b1bb6444101e15d9b325b82fa3283
      labels:
        app.kubernetes.io/name: mysql
        helm.sh/chart: mysql-9.4.4
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "8.0.31"
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: authentication-mysql
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: provider-dsc
                    app.kubernetes.io/name: mysql
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: mysql
          image: docker.io/bitnami/mysql:8.0.31-debian-11-r10
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: authentication-database-secret
                  key: mysql-root-password
            - name: MYSQL_DATABASE
              value: "my_database"
          envFrom:
          ports:
            - name: mysql
              containerPort: 3306
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          startupProbe:
            failureThreshold: 10
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/mysql
            - name: custom-init-scripts
              mountPath: /docker-entrypoint-initdb.d
            - name: config
              mountPath: /opt/bitnami/mysql/conf/my.cnf
              subPath: my.cnf
      volumes:
        - name: config
          configMap:
            name: authentication-mysql
        - name: custom-init-scripts
          configMap:
            name: authentication-mysql-init-scripts
  volumeClaimTemplates:
    - metadata:
        name: data
        labels: 
          app.kubernetes.io/name: mysql
          app.kubernetes.io/instance: provider-dsc
          app.kubernetes.io/component: primary
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: local-path
---
# Source: data-space-connector/charts/postgis/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: data-service-postgis
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: data-service-postgis
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgis-13.1.5
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: data-service-postgis-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: data-service-postgis
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: data-service-postgis
      labels:
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: data-service-postgis
        app.kubernetes.io/version: 16.0.0
        helm.sh/chart: postgis-13.1.5
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: provider-dsc
                    app.kubernetes.io/name: data-service-postgis
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:13.18.0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: data-service-secret
                  key: postgres-admin-password
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: custom-init-scripts
              mountPath: /docker-entrypoint-initdb.d/
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: custom-init-scripts
          configMap:
            name: data-service-postgis-init-scripts
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: local-path
---
# Source: data-space-connector/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgresql
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.0.0
    helm.sh/chart: postgresql-13.1.5
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: provider-dsc
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: postgresql
      labels:
        app.kubernetes.io/instance: provider-dsc
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 16.0.0
        helm.sh/chart: postgresql-13.1.5
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: provider-dsc
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:13.18.0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: database-secret
                  key: postgres-admin-password
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: custom-init-scripts
              mountPath: /docker-entrypoint-initdb.d/
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: custom-init-scripts
          configMap:
            name: postgresql-init-scripts
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: local-path
---
# Source: data-space-connector/templates/tmf-registration-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: tmf-api-registration
  namespace: "provider"
  labels:
    app.kubernetes.io/name: data-space-connector
    helm.sh/chart: data-space-connector-7.34.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    spec:
      containers:
        - name: register-credential-config
          image: quay.io/curl/curl:8.1.2
          command: [ "/bin/sh", "-c", "/bin/init.sh" ]
          volumeMounts:
            - name: tm-forum-registration
              mountPath: /bin/init.sh
              subPath: init.sh
      volumes:
        - name: tm-forum-registration
          configMap:
            name: tmf-api-registration
            defaultMode: 0755

      restartPolicy: Never
  backoffLimit: 10
---
# Source: data-space-connector/charts/apisix/templates/data-plane/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: provider-dsc-apisix-data-plane
  namespace: "provider"
  labels:
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: apisix
    app.kubernetes.io/version: 3.10.0
    helm.sh/chart: apisix-3.5.1
    app.kubernetes.io/part-of: apisix
    app.kubernetes.io/component: data-plane
spec:
  rules:
    - host: mp-data-service.127.0.0.1.nip.io
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: provider-dsc-apisix-data-plane
                port:
                  name: http
    - host: "mp-tmf-api.127.0.0.1.nip.io"
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: provider-dsc-apisix-data-plane
                port:
                  name: http
    - host: "rustapitest.127.0.0.1.nip.io"
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: provider-dsc-apisix-data-plane
                port:
                  name: http
---
# Source: data-space-connector/charts/odrl-pap/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: odrl-pap
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: odrl-pap
    helm.sh/chart: odrl-pap-0.2.2
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
  - host: "pap-provider.127.0.0.1.nip.io"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: odrl-pap
            port:
              number: 8080
---
# Source: data-space-connector/charts/scorpio/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: data-service-scorpio
  namespace: "provider"
  labels:
    
    helm.sh/chart: scorpio-0.4.7
    app.kubernetes.io/name: scorpio
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
  - host: "scorpio-federator.127.0.0.1.nip.io"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: data-service-scorpio
            port:
              number: 9090
---
# Source: data-space-connector/charts/tm-forum-api/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: provider-dsc-tm-forum-api
  namespace: "provider"
  labels:
    
    helm.sh/chart: tm-forum-api-0.10.10
    app.kubernetes.io/version: "0.13.2"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
  - host: "tm-forum-api.127.0.0.1.nip.io"
    http:
      paths:
      - path: /tmf-api/party/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-party-catalog
            port:
              number: 8080
      - path: /tmf-api/customerBillManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-customer-bill-management
            port:
              number: 8080
      - path: /tmf-api/customerManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-customer-management
            port:
              number: 8080
      - path: /tmf-api/productCatalogManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-product-catalog
            port:
              number: 8080
      - path: /tmf-api/productInventory/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-product-inventory
            port:
              number: 8080
      - path: /tmf-api/productOrderingManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-product-ordering-management
            port:
              number: 8080
      - path: /tmf-api/resourceCatalog/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-resource-catalog
            port:
              number: 8080
      - path: /tmf-api/resourceFunctionActivation/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-resource-function-activation
            port:
              number: 8080
      - path: /tmf-api/resourceInventoryManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-resource-inventory
            port:
              number: 8080
      - path: /tmf-api/serviceCatalogManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-service-catalog
            port:
              number: 8080
      - path: /tmf-api/agreementManagement/v4
        pathType: Prefix
        backend:
          service:
            name: provider-dsc-tm-forum-api-agreement
            port:
              number: 8080
---
# Source: data-space-connector/charts/trusted-issuers-list/templates/ingress-til.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: trusted-issuers-list-til
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: trusted-issuers-list
    helm.sh/chart: trusted-issuers-list-0.7.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "0.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
  - host: "til-provider.127.0.0.1.nip.io"
    http:
      paths:
      - path: /issuer
        pathType: Prefix
        backend:
          service:
            name: trusted-issuers-list
            port:
              number: 8080
---
# Source: data-space-connector/charts/vcverifier/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: verifier
  namespace: "provider"
  labels:
    
    app.kubernetes.io/name: vcverifier
    helm.sh/chart: vcverifier-2.13.0
    app.kubernetes.io/instance: provider-dsc
    app.kubernetes.io/version: "2.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "provider-verifier.127.0.0.1.nip.io"
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: verifier
              port: 
                number: 3000
